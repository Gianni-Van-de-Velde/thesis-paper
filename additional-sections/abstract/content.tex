
\titleformat{\chapter}{}{}{0em}{\bf\Huge}
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}


In recent years, Large Language Models (LLMs) have taken over the world, leading to spectacular applications but also concerns about its energy consumption. In an ideal world, the models would provide the same answers faster, while using less energy. This thesis will prove we live in that ideal world. Speculative decoding is a proven method to increase time and energy efficiency for LLMs and this thesis builds further on the current state-of-the-art (SOTA) to achieve even more efficiency. We present Context-Aware Speculative Decoding (CASD), a hybrid method that can augment any other speculator to achieve better performance in high-copy environments. Retrieval-Augmented Generation (RAG) is a technique that is very popular in the business world and it yields such a high-copy environment. However, limited benchmarks are available for RAG and certainly in niche domains (e.g. medical, Dutch), where the SOTA underperforms. For this reason, we deliver a new benchmark UZGentRAG with data from a real world use case: RAG at UZGent. This benchmark not only proves the strength of CASD, but also the gap that current method seem to leave open. In the future, we expect CASD to become the standard augmentation method of speculative decoders in RAG uses cases, certainly in low-resource languages.

\section*{Keywords}
\{Speculative Decoding, RAG, LLM\}
