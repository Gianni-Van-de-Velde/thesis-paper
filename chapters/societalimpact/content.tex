
\chapter{Societal impact}
\label{sec:societal_impact}

In this thesis, multiple societal aspects are touched. Firstly, our technique (CASD) seeks to reduce the energy consumption of LLMs in specific use cases. Secondly, the same method reduces the latency of LLM generation, the impact of which can range widely. Recently, speculative decoding has also been associated with the democratization of LLMs. Finally, the use case at the UZGent is intended to improve efficiency in healthcare.

\section{Reduced energy consumption of LLMs}
We are currently living in a climate-crisis, of which climate change is a major component. This climate change is caused by greenhouse gas emissions, where between 1.5\% and 4\% is attributed to the ICT sector \cite{ayers2025measuring}. This fraction is already quite large and AI is expected to drive  up emissions even more. For example, datacenter electricity demand is expected to more than double by 2030, with AI as the largest cause \cite{iea2025ai}. Needless to say, reducing the energy consumption by AI and thus LLMs will have quite some societal impact. % This aligns with SDG 13: climate action.

This thesis strives towards the goal of using less energy with LLMs. But does it succeed? Our new method, CASD, successfully reduced the energy consumption of an LLM in multiple experiments. This offers a tool to LLM implementers to generate the same answer with less energy, but that does not mean we succeeded yet. To actually have a positive societal impact, the following two requirements need to be met. Firstly, the method has to be adopted, because there is no use to our tool when it is not used. Secondly, we must account for Jevons' paradox: making a tool to reduce the energy per usage might stimulate more usage and result in no net gains \cite{alcott2005jevons}. So only time can tell whether the impact on greenhouse gases will be positive.

In this paragraph, we add some technical details to the previous paragraph. First of all, CASD only yields significant energy reduction for RAG use cases. On top of that, batching is preferred over speculative decoding when there is enough throughput, so the energy reduction is only for low throughput applications. With the previous notes in mind, a local RAG system is clearly the perfect use case for CASD, as local systems typically low throughput. As RAG is very popular and local LLMs are also gaining traction, the potential impact is still quite large \cite{espin2025rise, k2view2025GenAI, pieces2025local, menlov2025state}. 

\section{Reduced LLM latency}

Reducing the latency of an LLM is a double-edged sword. On the one positive side, it can improve the user experience, because they have to wait less long for their answer \cite{bamoria2025solving}. Also, this allows for complexer application pipelines that require more LLM steps, such as the recently popular agents \cite{chudleigh2025complete, suard2025quick}. However, that already shows the negative side too: it can stimulate more usage. When an answer is generated faster, more answers can be generated, leading to Jevons' paradox. % This has the risk to work against SDG 13.

Again, we add some technical sidenotes. Whether there is positive impact and negative impact depends on how speculative decoding is used. If speculative decoding is applied in a low throughput application, it uses the GPU more efficiently by exploiting the spare compute, leading to less energy consumption \cite{qin2024optimized}. However, large scale LLM providers can also use speculative decoding \cite{hassabis2025year, svirschevski2024specexec}. This leads to faster answers, but more energy consumption. That is because a trade-off is made between batching and speculative decoding, where batching is even more efficient for energy than speculative decoding \cite{fernandez2025energy}. For this reason, \textbf{we advise to always prefer batching over speculative decoding}, until the optimal input size for the used GPU is reached. In conclusion, the answer will always come faster with speculative decoding, but whether energy gains are made depends one how it is used.

\section{Democratization of LLMs}
For the most efficient text generation using LLMs, the whole model should be loaded into GPU VRAM \cite{chen2024practical}. Unfortunately though, GPUs that can load (let alone train) modern LLMs are typically very expensive. This has even led to jokes about two new classes in society: the GPU-Poor and the GPU-Rich \cite{envisioning2025gpu}. So, clearly there is a need for democratization of LLMs such that anyone can run their own LLM, not only the GPU-Rich.

Speculative decoding can also help in the democratization of LLMs. Recent advancements in the domain showed that running LLMs on much wider available, cheaper GPUs is also feasible \cite{svirschevski2024specexec}. While the generation speed is not the same as for the specialized GPUs, it at least allows the GPU-Poor to also run their own LLMs.

Running an LLM on smaller VRAM works as follows. The LLM is loaded into RAM, which is typically much larger than VRAM. Then, only the necessary part of the LLM is loaded into VRAM for a specific part of the LLM calculation and that part is executed. This is repeated for all steps, until the final LLM output is generated. That technique is called offloading and while it works, the transfer to VRAM usually takes prohibitively long. SpecExec \cite{svirschevski2024specexec} suggests speculating up to thousands of tokens to alleviate this issue. They show that even thousands of speculations can be decoded with negligible overhead, because the bottleneck is in the loading of parameters into VRAM. We hypothesize that CASD can also augment this method well, because this method needs very long high quality speculations.

\section{Improved healthcare efficiency}
Computers have modernized healthcare in more ways than we can sum up here, ranging from medical imaging to electronic health records. This modernization is a continuing process, further boosted by the recent speed of AI advancements. Yet, that speed is hard to follow and even staying up to date with all the possibilities is quite hard. In dialogue with the CTO of UZGent, we found that bringing UZGent up to speed was the most impactful effect of this thesis. By that we certainly do not mean that a production-ready AI application was deployed, but rather that we started with the first steps towards responsible AI usage. The first step is always understanding what the AI cannot do. Thus, the emphasis was on explaining hallucinations and that making a chatbot that answers correctly 100\% of the times is still impossible today. The next step is to know what the AI can do. The RAG system that was set up helped in both steps, because it showed hallucinations, but also it impressed by its retrieval capabilities. As a third step, we also initiated data preparation with the goal of AI evaluation. Now, when an AI implementer approaches UZGent with a proposal to work together, they have already prepared a risk-free sample dataset on which the implementer can train, test etc. Next to that, we also built a validate and test set, such that UZGent can evaluate and compare different AI implementers in how well their retrievers work. In conclusion, the RAG system might never see production, but it was a great demo to increase awareness and preparedness for AI at UZGent.

% As this part of the thesis was mostly about healthcare and trying to improve the efficiency of doctors with AI, we associate it with SDG 3: good health and well-being.
