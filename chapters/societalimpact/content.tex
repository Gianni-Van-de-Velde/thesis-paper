
\chapter{Societal impact}
\label{sec:societal_impact}

Until now, this thesis focused on the academical value of our research, but we think it is important to also evaluate the potential societal impact. To do so, this chapter lists four different effects that our research can have, and we evaluate whether this could have a positive or negative impact.

\section{Reduced energy consumption of LLMs}
We are currently living in a climate-crisis, of which climate change is a major component. This climate change is caused by greenhouse gas emissions, where between 1.5\% and 4\% is attributed to the ICT sector \cite{ayers2025measuring}. This fraction is already quite large and AI is expected to drive  up emissions even more. For example, datacenter electricity demand is expected to more than double by 2030, with AI as the largest cause \cite{iea2025ai}. All this to say that reducing the energy consumption by AI and thus LLMs could be a small part of tackling the climate-crisis we live in.

This thesis strives towards the goal of using less energy with LLMs. Our new method, CASD, successfully reduced the energy consumption of an LLM for RAG in multiple experiments. However, we must also take into account the secondary effects of this. Jevons' paradox \cite{alcott2005jevons} says that while CASD improves the energy efficiency, it might lead to more energy consumption through increased demand. So, we can only urge implementers of CASD and LLMs in general to use the technologies responsibly, with the energy consumption in mind.

As we mentioned responsible use, it is important to define what that means. First of all, the general LLM guidelines still hold.
\begin{itemize}
    \item Is the answer of the LLM worth the energy consumption?
    \item Does this task/application actually need an LLM or do more efficient method exist?
    \item Is a more energy efficient (L)LM fit for this job?
\end{itemize}
Next to that, an implementer of an LLM can use these extra technical guidelines.
\begin{itemize}
    \item When possible, use specialized tools such as vLLM \cite{kwon2023efficient} for inference. Such tools apply batching and speculative decoding to generate answers faster with less energy.
    \item Always use as much batching as possible and then fill the remaining GPU-power with speculative decoding, as this uses the least energy \cite{fernandez2025energy}. This is because with batching each decoded token is used, while in speculative decoding only part of the decoded tokens are accepted.
\end{itemize}
This last point already indicates that speculative decoding is more important when batching is not (or hardly) possible. This is typically the case for local LLMs, as there are not enough users to fill the GPU-power with batching only. A local LLM is an LLM that a company decides to run on their own servers. This has the advantage that the data (question, answer and documents for RAG) never leaves the company and thus it is the preferred method for handling more sensitive data. For this reason, local RAG systems are becoming more popular \cite{espin2025rise, k2view2025GenAI, pieces2025local, menlov2025state}, and CASD just so happens to reduce energy most for these applications.

\section{Reduced LLM latency}

Reducing the latency of an LLM is a double-edged sword. On the one positive side, it can improve the user experience, because they have to wait less long for their answer \cite{bamoria2025solving}. Also, this allows for complexer application pipelines that require more LLM steps, such as the recently popular agents \cite{chudleigh2025complete, suard2025quick}. However, that already shows the negative side too: it can stimulate more usage. When an answer is generated faster, more answers can be generated, leading to increased energy consumption. This is already happening with large-scale LLM providers, who use speculative decoding over batching, which is more energy efficient \cite{hassabis2025year, svirschevski2024specexec}. This type of speculative decoding then powers applications that would otherwise be unpopular due to the long latency, such as some reasoning LLMs, where many more tokens are generated than usual. This is why we advise again to always prefer batching over speculative decoding for optimal energy efficiency.

\section{Democratization of LLMs}
With LLMs becoming larger each year, the primary concern is energy consumption. Yet, in the shadow of this, another issue is often overlooked: the elitism of LLMs. 

The state-of-the-art LLMs today are about a 1000 times larger than the state-of-the-art LMs were in 2018 (comparing DeepSeek V3 with BERT) \cite{devlin2019bert, liu2024deepseek}. Needless to say, you also need much more expensive GPUs to run such models. This has even led to jokes about two new classes in society: the GPU-Poor and the GPU-Rich \cite{envisioning2025gpu}. So, clearly there is a need for democratization of LLMs such that anyone can run their own LLM, not only the GPU-Rich.

Speculative decoding can also help in the democratization of LLMs. Recent advancements in the domain showed that running LLMs on much wider available, cheaper GPUs is also feasible thanks to speculative decoding \cite{svirschevski2024specexec}. While the generation speed is not the same as for the specialized GPUs, it at least allows the GPU-Poor to also run their own LLMs. Our work could further improve the speculations, making it possible to run the LLMs on even cheaper hardware.

\section{Improved healthcare efficiency}
The last point on societal impact is about the use case at UZGent of Chapter \ref{sec:use_case_uzgent}. There, we tried to improve the efficiency of the healthcare professionals with a RAG application, because we want them to spend less time searching documents in a database and more time helping patients. Fully achieving this goal was of course out of scope for this thesis, but it was a good start of a project that will continue beyond this thesis.

In dialogue with the CTO of UZGent, we found that bringing UZGent up to speed was the most impactful effect of this thesis. Next to the technical implementation of the RAG system, we put significant effort in creating awareness about what AI can and cannot do. UZGent now also feels more ready to step into the AI world, as they have a test set prepared to compare different AI vendors, who promise to deliver the best retrieval system.
