
\chapter{Speculative Decoding}

The academic part of this thesis revolves around speculative decoding. This chapter provides details around speculative decoding and the current landscape of research. This serves as context for the next chapter, where a new model is proposed. First the principles of speculative decoding are explained. Then an overview is given of the most recent methods. Finally, one model is chosen to work as a baseline for the rest of the research.

\section{Context of speculative decoding?}
To understand the need for speculative decoding, one must first understand the usual way of decoding with LLMs. To start, consider an LLM host, that only occasionally serves a user asking a question. Each time a question comes in, the LLM generates an answer autoregressively. Autoregressive means that the LLM "reads" all previous tokens to generate the next token. So to construct an answer, the LLM takes the input and generates one new token. Then it takes the input and the new token and generates another new token. This is repeated until the LLM decides the answer is complete or the maximal output length is reached.

The issue with this autoregressive decoding is that it does not use the full capacity of the GPU it is running on. Oversimplifying what GPUs do, there are two constraints that limit the speed with which a GPU can run an LLM. On the one hand, a GPU has limited computing power, which means it can only do so many operations per second. On the other hand, GPUs can load a limited amount of data per time. Surprisingly, the latter constraint will be the real bottleneck for the LLM host from before. This is because each time the LLM is called (so many times per answer), the LLM parameters need to be loaded from the VRAM to the GPU. This large amount of data seems to be slower to transfer than the actual time the GPU needs to compute the results. In other words, the GPU could calculate more, but loading the model each time keeps it from doing so.

There are two types of solutions for this bottleneck. The first one is relevant to the larger players in the market. Consider the LLM host from before, but now it receives many requests per second. In this case, the questions can be batched. This means that the server waits for a certain number of questions to arrive, before processing them all together, in parallel. The effect is that the model is only loaded once into the GPU, while generating the next token for each of the questions. So if there are for example 16 questions in a batch, the GPU is calculating 16 times as much, while loading the same model only once. Thus, the calculation intensity is increased and much of the spare calculation-capacity is filled in. In conclusion, this method increases the GPU efficiency by batching multiple questions, by exploiting the many requests coming in per second.

The second solution to the GPU memory bottleneck is speculative decoding. Speculative decoding does not need the large load of requests per second to achieve parallellism. Instead, it tries to break the autoregressive nature of LLMs and to parallellize it. But how does that work? First, guesses are made of what the LLM will generate. Then the guesses are verified in parallel. Finally, the longest correct guess is accepted. This is also repeated until a stop condition is met. This is still autoregressive, one might say, but there are possibly many tokens generated per iteration thanks to the parallellism. Each step is detailed more below.

\section{How does speculative decoding work?}

The first step is to make a good guess what the LLM will generate, without calling the LLM itself. This step is the hardest and by consequence the focus point of most research in the domain. Many clever methods have been found already. There are three fundamentally different categories of methods to make guesses. One category contains the statistical methods. REST is one of the statistical models. The second category is the model based methods. EAGLE-2 is one of those methods. The third category is the LLM based methods.

In the second step, the guesses are verified in parallel. To do so, a tree is made of all predictions. In this tree, guesses with a common prefix are joined, such that the prefix has to be verified only once. An example of such tree is shown in Figure \ref{TODO}. Once the tree is made, each node of the tree needs to be ran through the LLM, to see what the correct token is at that step. When sending all tokens to the LLM, it must be sure that tokens from different guesses do not interfere with each other. This is made possible with tree attention. Tree attention puts a mask over the attention, such that tokens from other branches in the tree are "invisible" to the current branch. So to conclude, tree attention makes it possible to call the LLM only once, while verifying many guesses.

The third step is to accept the longest correct guess. A "correct guess" can have multiple interpretations. When the temperature is 0, it is easier to define. Temperature 0 means that the LLM will always, deterministically pick the most probable next word. In this case, correct means that the guess chose exactly the same tokens as the LLM. In the case of temperature different from 0, the answer is not deterministic anymore. So more calculations are involved to guarantee the output distribution is the same as the LLM would have chosen. As this sampling method is not the focus of this thesis, it is not further explained. One last remark must be made to fully understand the accept step. The real accepted length is always one larger than the longest accepted guess. How is that possible? The LLM was run for each node in the tree. So for each step, it is known what the correct token was, regardless of the guesses made. So that means the accepting can start at the root. If the next token is guessed correctly, great, accept it and move to that node. If it is not guessed correctly, accept the correct token that was not guessed. This last part ensures one extra token is accepted each iteration.

