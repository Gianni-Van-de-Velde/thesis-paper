
\chapter{Conclusions and future work}
\label{sec:conclusion_future_work}

In this thesis, we introduced CASD, a model-agnostic augmentation to any modern speculator for speculative decoding. Multiple benchmarks proved its performance in RAG scenarios, while overhead for other scenarios remains limited. We introduced one of those benchmarks ourselves; UZGentRAG is made of real-world questions that healthcare professionals asked a RAG chatbot, which we implemented at UZGent. To summarize the findings, we answer the research questions posed in the introduction:

\begin{tcolorbox}[colback=blue-ish-light,colframe=blue-ish,title=\textbf{RQ1:} {{Can a new speculative decoding strategy be designed, relying on the repetition of phrases in prompt and answer, resulting in increased inference efficiency of LLMs?}}, coltitle=white]
    Yes it can. In RAG use cases, our new method CASD successfully improves speculative decoding, by drafting copies from the context. As UZGentRAG and SQuAD are benchmarks of the intended use cases, only these are mentioned below.
    \begin{itemize}
        \item \textbf{RQ 1.1} Which energy gains does CASD make? CASD makes energy gains of 17\% on UZGentRAG and 7\% on SQuAD.
        \item \textbf{RQ 1.2} Which speedups does CASD deliver? The speedups are 1.17 for UZGentRAG and 1.08 for SQuAD.
    \end{itemize}
\end{tcolorbox}

Exploring this question led to some interesting insights. We found that the current state-of-the-art, EAGLE-2 already has some notion of copying implicitly learned. However, CASD adds much longer high quality drafts from the context with a high probability of being accepted. This is why CASD improves speculative decoding strongly in RAG use cases. Yet, in standard non-RAG settings, the performance went down as the LLM barely copies from the context. In the end, the key takeaway from this question is that fundamentally different speculators can be combined to transcend the speculators separately. On a personal note, my key takeaway is that even very simple ideas like CASD can be successful, even when the current state-of-the-art is so complex and seems unbeatable. 

\begin{tcolorbox}[colback=blue-ish-light,colframe=blue-ish,title=\textbf{RQ2:} Does a real-world benchmark in a niche domain show significantly different results from existing speculative decoding benchmarks?, coltitle=white]
    Yes, the results from our new private UZGentRAG benchmark differ significantly from SQuAD and even more from the non-RAG benchmarks. This shows a gap of niche benchmarks in the current set of public benchmarks.
\end{tcolorbox}

The answer to this question is important, not only for the academic world, but also for real-world applications. As we show that the performance of speculators can differ significantly for a niche domain, a one-size-fits-all approach with speculators might not be optimal. While CASD is specifically made for RAG, it is domain-agnostic of the content of that RAG system, solving a part of this difficulty.

\begin{tcolorbox}[colback=blue-ish-light,colframe=blue-ish,title=\textbf{RQ3:} Can a RAG system provide additional value at UZGent?, coltitle=white]
    Partially. The retriever has potential to save healthcare professionals much time that they could spend helping their patients. Yet, the generation step of RAG is currently no added value and it will probably require significant time, effort and money to extract value from such generator.
\end{tcolorbox}

While the full RAG pipeline will probably not provide value for UZGent in the foreseeable future, working with AI there opened up interesting discussions. We found that UZGent, as probably many hospitals, has untapped potential to use AI in an ethical and safe manner. We believe that AI-based retrievers are the low-hanging fruit for multiple applications at UZGent, while being ethical and safe.

In the future, we see great potential for CASD to augment other speculators in RAG use cases, because of its cheap, high quality, long length speculations. The future might bring even more advanced strategies around the idea of CASD. This is the future work we see for CASD:
\begin{itemize}
    \item Applying CASD to the baseline's speculated tokens. CASD currently works completely independent of the baseline speculator and merges the speculation trees afterwards. However, the results showed that EAGLE-2 was already reasonably good at copying shorter spans. If CASD could pick up on those shorter spans and expand on them, that could lead to even longer high quality speculations.
    \item Taking into account speculator confidence. Some speculators (e.g. EAGLE-2) have a built-in confidence metric, which can help decide which tokens to speculate. CASD does not have that, but in Section \ref{sec:further_analysis}, we showed that combining match length with prediction depth provides a good heuristic for confidence. Combining this information, we can keep only the top-k speculations according to confidence. This allows for optimizing k to the specific hardware used, which can lead to significant gains in efficiency \cite{fernandez2025energy}.
    \item Optimization with CUDA kernels. The current implementation is not overly optimized. So, it is likely that further optimizations such as specialized CUDA kernels could help CASD achieve even higher speedups.
\end{itemize}

Also for the use case at UZGent we summarize the future work:
\begin{itemize}
    \item On the practical side, we suggest that UZGent focuses on evaluation rather than implementation. UZGent is a hospital and its IT department works hard to make the life of their healthcare professionals easier. Developing in-house AI solutions is likely beyond the scope of that task. What seems more realistic is that the IT department builds up a good framework to evaluate all AI solution providers and to see which solution benefits their healthcare professionals the most. The test set for retrieval we made together is certainly a step in the right direction. As the retrieval of current systems is subpar compared to the possibilities of modern AI, we suggest to start there.
    \item On the technical side, many possibilities remain unexplored. The low-hanging fruit for the data in Zenya, might be to add metadata to the queries. However, fine-tuning the AI models seems to have the largest potential impact, as current embedders get confused by terms that were not part of their training data.
\end{itemize}
