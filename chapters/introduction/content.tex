\chapter{Introduction}
In recent years, the quality of LLMs has made significant progress. With it, the widescale usage of LLMs has grown so much, up to the point that its energy consumption has raised major concerns. The larger players in the market even started building nuclear plants, just to be able to keep powering the servers that host these LLMs. Or to look at it from another perspective: each time a user asks a question to a modern LLM, so much energy is used on the server as one would need to charge their phone from 0\% to 100\%. So it is clear that the adoption of LLMs in the daily life should come with extra steps to counter the explosion of energy consumption. One component of the solution is to use fewer LLMs. While often neglected, this may have the largest impact. Do we actually need a SOTA LLM to generate us a poem, to have a chat with or to suggest which movie to watch? Another part of the solution is to make sure the least possible energy is used to generate the same quality of the response. But how would that work?

Speculative decoding, a proven method, promises to generate the same answers faster, using less energy. It works by using a small model to draft many samples of what the generator could generate in the coming tokens. Then, these drafts are verified in parallel by the LLM. This parallelisation can lead to speedups of up to 4x. And, as will be explained later, this even leads to decreased energy consumption \cite{qin2024optimized}. While speculative decoding is not new, there is much freedom in how the speculator (the draft model) works and this is where current research focuses on.

\textbf{The main goal of this thesis is to develop a new speculator CASD, that augments the current SOTA to increase efficiency of LLMs in high-copy environments.} With the popularity of RAG, such high-copy environments become more and more common and increasing LLM efficiency for those cases has the potential to make a large impact. To prove that a new method works, relevant benchmarks are necessary, but the number of RAG benchmarks is very limited. Even more, only one well known English benchmark exists, that can be adapted to be a RAG benchmark for speculative decoding. To fill the gap in more niche benchmarks, \textbf{the second goal is to make a novel benchmark UZGentRAG, that contains real-world questions and context to evaluate speculative decoders on.} To gather realistic data, one needs a live RAG system that actual doctors can direct their questions to. As the UZGent is interested in a RAG application, \textbf{the third goal is to make a functioning RAG system at the UZGent}, more as a side quest. 

With these goals in mind, the following research questions follow naturally.

\begin{itemize}
    \item \textbf{RQ 1}: Could context improve speculative decoding? \\ RQ 1.1 Which energy gains does CASD make? \\ RQ 2.2 Which speedups does CASD deliver?
    \item \textbf{RQ 2}: Does a real-world benchmark in niche domain show significantly different results from existing benchmarks?
\end{itemize}

In this paper, the context is built up logically, before diving into the research questions. To do so, the structure is as follows. TODO this is not mine. First, in chapter \ref{TODO UZ case} UZ case. Then, in chapter \ref{TODO RAG} RAG. Chapter \ref{TODO speculative decoding}. In chapter \ref{TODO CASD}. Then in \ref{TODO results} the results of these trials. Chapter \ref{conclusion_future_work} formulates answers to the research questions and discusses the most important opportunities for future work.\\