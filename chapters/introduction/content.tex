\chapter{Introduction}
In recent years, the quality of Large Language Models (LLMs) has made significant progress \cite{zhao2023survey}. With it, the widescale usage of LLMs has grown so much, up to the point that their energy consumption has raised major concerns \cite{alizadeh2024analyzing, bender2021dangers, strubell2020energy}. The larger players in the market even started building nuclear plants, just to be able to keep powering the servers that host these LLMs \cite{luscombe2024three, wnn2025facebook, bbc2025ai}. Or to look at it from another perspective: each time a user gives a long prompt to a modern LLM, so much energy is used on the server as one would need to charge their phone from 0\% to 100\% \cite{epoch2025energy}. So it is clear that the adoption of LLMs in the daily life should come with extra steps to counter the explosion of energy consumption. One component of the solution is to use fewer LLMs. While often neglected, this may have the largest impact \cite{alcott2005jevons}. Do we actually need a SOTA LLM to generate us a poem, to have a chat with or to suggest which movie to watch? Another part of the solution is to make sure the least possible energy is used to generate the same quality of the response. But how would that work?

Speculative decoding, a proven method, promises to generate the same answers faster, using less energy \cite{leviathan2023fast, qin2024optimized}. It works by using a small model to draft many samples of what the generator could generate in the coming tokens. Then, these drafts are verified in parallel by the LLM. This parallelization can lead to speedups of up to 4x \cite{li2024eagle}. And, as will be explained later, this even leads to decreased energy consumption \cite{qin2024optimized}. While speculative decoding is not new, there is much freedom in how the speculator (i.e. the draft model) works and this is where current research focuses on.

\textbf{The main goal of this thesis is to develop a new speculator, CASD, that augments the current SOTA to increase efficiency of LLMs in high-copy environments.} With the popularity of RAG \cite{k2view2025GenAI, menlov2025state}, such high-copy environments become more and more common and increasing LLM efficiency for those cases has the potential to make a large impact. To prove that a new method works, relevant benchmarks are necessary, but the number of RAG benchmarks is very limited. Even more, we found only one well known English benchmark, that can be adapted to be a RAG benchmark for speculative decoding \cite{rajpurkar2016squad}. To fill the gap in more niche benchmarks, \textbf{the second goal is to make a novel benchmark, UZGentRAG, that contains real-world questions and context to evaluate speculative decoders on.} To gather realistic data, one needs a live RAG system that actual healthcare professionals can direct their questions to. As the UZGent is interested in a RAG application, \textbf{the third goal is to make a functioning RAG system at the UZGent}, as a case study. 

With these goals in mind, the following research questions follow naturally:

\begin{tcolorbox}[colback=blue-ish-light,colframe=blue-ish,title=\textbf{Research Questions}, coltitle=white]
    \textbf{RQ1:} Could context improve speculative decoding?
    \begin{itemize}
    \item RQ1.1: Which energy gains does CASD make?
    \item RQ1.2: Which speedups does CASD deliver?
    \end{itemize}
    \textbf{RQ2:} Does a real-world benchmark in a niche domain show significantly different results from existing speculative decoding benchmarks? \\
    \textbf{RQ3:} Can a RAG system provide additional value at UZGent? \\
\end{tcolorbox}

In this paper, the context is built up logically, before diving into the research questions. The structure is as follows: First, in Chapter \ref{sec:rag}, we summarize the relevant literature around RAG for the UZGent use case. Chapter \ref{sec:use_case_uzgent} applies this knowledge in the practical implementation of the RAG system. Chapter \ref{sec:speculative_decoding} describes speculative decoding, which is the main academic topic of this thesis. In Chapter \ref{sec:casd} we then introduce our new method, CASD, to improve speculative decoding. This is followed by Chapter \ref{sec:conclusion_future_work}, which formulates answers to the research questions and discusses the most important opportunities for future work. Finally, Chapter \ref{sec:societal_impact} looks in to the societal impact these results can have.