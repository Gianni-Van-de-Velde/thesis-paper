\section{Introduction}
In recent years, the quality of LLMs has made significant progress. With it, the widescale usage of LLMs has grown so much, up to the point that its energy consumption has raised major concerns. The larger players in the market even started building nuclear plants, just to be able to keep powering the servers that host these LLMs. Or to look at it from another perspective: each time a user asks a question to a modern LLM, so much energy is used on the server as one would need to charge their phone from 0\% to 100\%. So it is clear that the adoption of LLMs in the daily life should come with extra steps to counter the explosion of energy consumption. One component of the solution is to use fewer LLMs. While often neglected, this may have the largest impact. Do we actually need a SOTA LLM to generate us a poem, to have a chat with or to suggest which movie to watch? Another part of the solution is to make sure the least possible energy is used to generate the same quality of the response. Broadly speaking, this thesis focuses on the latter topic: how can an LLM generate the same answer, while reducing the energy consumption?\\

The goal of this thesis is threefold. Firstly, an LLM is employed to boost efficiency in UZGent. More specifically, a RAG system will allow healthcare professionals to chat with the internal "Zenya" database. (TODO schrijf do we actually need?) Secondly, SOTA techniques are used to keep the energy consumption down as much as possible while using the LLM. Speculative decoding is a technique that perfectly works on a scenario like that for the UZGent. This technique does not only increase energy efficiency, but it also reduces the time to wait before an answer is generated. Thirdly, this thesis builds further on the SOTA, using application-specific knowledge. Larger research groups are focused on beating the previous SOTA with 0.01\% so to speak, in a very general case. On the contrary, this research tries to make larger gains in a real world scenario, by using the specific properties of the case for which the LLM is used. The most important difference with the general case is that the LLM operates in a RAG framework. Here the expected input-output behaviour is different from the general case, allowing niche techinques to improve on models trained for the general case.\\

From the use case and the mentioned research topics, the research questions follow naturally.

\begin{itemize}
    \item \textbf{RQ 1}: TODO
\end{itemize}

In this paper, the context is built up logically, before diving into the research questions. To do so, the structure is as follows. TODO this is not mine. First, in chapter \ref{TODO UZ case} UZ case. Then, in chapter \ref{TODO RAG} RAG. Chapter \ref{TODO speculative decoding}. In chapter \ref{TODO CASD}. Then in \ref{TODO results} the results of these trials. Chapter \ref{conclusion_future_work} formulates answers to the research questions and discusses the most important opportunities for future work.\\