
\chapter{Use case UZGent}
To put the theory of speculative decoding into practice, this thesis looks at a use case for RAG at the UZgent. This chapter starts with a description of the use case. Then the technical implementation is uncovered, detailing how the architecture of the RAG system works.

\section{Description of use case}

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{fig/zenya.jpeg}
	\caption{Screenshot of the current system doctors use to search procedure documents: Zenya.}
	\label{fig:zenya}
\end{figure}

In the UZGent, there is a system, Zenya and it contains many documents often needed by healthcare professionals. To work with the tool, one must either be guided or have enough experience themselves in order to get the document they need to read. Also, the documents can be quite lengthy, while often only a very specific part or summary is necessary. This makes the perfect use case for a RAG system. In this RAG system, the user first poses a question. Then the retrieval fetches the relevant documents based on a deeper understanding than the current keyword-only search. Finally, the relevant part of the document is read by the LLM and used to answer the question in a concise way. This description was rather general about RAG, however each use case is different and has different requirements. To get a grasp of the details of this use case, many involved people were interviewed. A summary is given below.

The answer to each question is in one single document. This already simplifies the RAG complexity. It also means that the retriever has only one document to find as the ground truth. However, this can still be multiple chunks long, sometimes requiring a summary of large parts of the document.
The question will be in natural language, rather than keywords as it used to be with Zenya. However, the language can contain abbreviations, short rather incomplete sentences. This is because the healthcare professional often needs the answer quickly.
The system should know when it does not know. It is very important that when there is no answer in the database, it is able to detect this and answer accordingly.

\section{Technical implementation}
In this section, the practical choices are listed and why they were made. Following chronological order, the documents need to be preprocessed first. Then, the retriever must find the right chunks and the generator will produce an answer. Finally, some postprocessing will make the result more accessible to the end user. As a guide through this section, Figure \ref{fig:dataflow_graph} shows the high level data flow graph that makes RAG possible.

\begin{figure}[H]
    \centerline{\includegraphics[width=1\linewidth]{fig/dataflow_graph.png}}
    \caption{The data flow graph, which shows how the data and input question lead to a final answer. The graph is visually split vertically, to indicate the difference in time. The left part fills the database and as this data is constant, it can be calculated in before. So practically, this full preprocessing is done before the system goes live. The right part shows the live calculations that need to happen, being repeated for each new question that comes in.}
    \label{fig:dataflow_graph}
\end{figure}

\subsection{Preprocessing}
To start the preprocessing, one must first know what format the data will have. The database of Zenya contained many file formats: pdf, docx, excel, ... This heterogenity is not desirable for building a quick POC in the scope of this thesis. Yet, it seemed that the majority of the files were pdfs and docx, most of which contained primarily text. This is visualized in Figure \ref{fig:file_type_distribution}. The first conclusion is that pdfs and docx are important, while the rest can be ignored in a simple POC. Also, the data exporter made it possible to convert docx to pdf and for simplicity, this was used, such that the input data was as homogeneous as possible. The python module PyPDFDirectoryLoader of the langchain\_community package  provides very concise code to parse these pdfs and provide the text in them. This text is not directly used, because pdfs (like most other formats) contain much textual noise. One example is the character "$\backslash$" which often appears in places one cannot find visually inspecting the pdf. For this reason, a pdf text cleaner scans all text and strips the text down to the real content. After this cleaning, mostly natural language remains in the text. To clarify, an example is given in Figure \ref{fig:pdf_clutter}

\begin{figure}[H]
    \captionsetup{justification=centering}
    \centerline{\includegraphics[width=0.7\linewidth]{fig/file_type_distribution.png}}
    \caption{The file type distribution of Zenya documents.}
    \label{fig:file_type_distribution}
\end{figure}

\begin{figure}[H]
    \centerline{\includegraphics[width=0.7\linewidth]{fig/pdf_clutter.png}}
    \caption{An example page of a pdf document in Zenya. For any human it is clear that the only text on this page is ``NOTA'S / 15'' in the top right. Yet the pdf parser translates this to ``/  15$\backslash$nNOTA'S$\backslash b \backslash b \backslash b \backslash b \backslash b \backslash b$...''. Here the $\backslash$n stands for a new line, $\backslash b$ stands for the backspace character and the ... hides another 2500 times the $\backslash b$ character. Most likely, the lines were translated to the special character $\backslash b$.}
    \label{fig:pdf_clutter}
\end{figure}

With the actual text available, the next steps are those typical to RAG. 

\subsection{Chunking}
First, the text of each pdf is split into smaller pieces, chunks. This chunking helps in multiple ways. Good chunking can make the pieces of text more bite-sized, such that the embedding model can interpret it better. Also, RAG does not always need to read the full document, but rather the relevant part, so chunking also limits the amount of noise around the retrieved information. There are some choices necessary to do chunking successfully. The optimal chunk size, the first choice, depends on both the dataset and the embedder. It is usually helpful to match the chunk size with how long one thought is in the dataset. Next to that, an embedder is trained on a certain input size and they work better when working in similar sizes of new input. Empirically, it was found that approximately 800 characters is optimal for the use case. 

The next option, is to use overlap. When splitting text it is impossible not to break an ongoing thought at any point. To make sure this cut in the text does not lose the information in that passage, overlap is often used to make sure this context helps both the previous and following chunk. The exact number did not seem to matter enough to make significant changes to the validate score, so we chose a 20\% or 160 characters overlap.

\begin{figure}[h]
    \captionsetup{justification=centering}
    \centerline{\includegraphics[width=1\linewidth]{fig/good_split_bad_split.png}}
    \caption{The difference between a good and bad point to split, applied on content from Zenya.}
    \label{fig:good_split_bad_split}
\end{figure}

With this target chunk size and overlap, one still needs to decide where to cut the text. While deterministically stepping by 800 characters at a time would be a simple method, it can cut through thoughts, or even worse: words. Figure \ref{fig:good_split_bad_split} shows the difference between a good and a bad split. To solve this, the RecursiveCharacterTextSplitter of langchain\_text\_splitters splits the text recursively. Each time, it tries to find the most splittable characters until the desired chunk size is approximately met. Practically, this means that a hierarchy is followed where, for example, the new line character "$\backslash$n" is prefered over a space " " to split on. With this last choice made, the chunks can be extracted.

\subsection{Retrieval}
With the text split into chunks, the next step is to retrieve the right chunks for a query. This is the hardest part for a reasonable proof of concept. A clear distinction is made between what worked and what did not.

\subsubsection{Effective retrieval techniques}
In this part, the retrieval techniques are listed that are actually used in the final RAG system. In other words, the methods below all successfully made the retriever return better documents.

As explained in Chapter \ref{sec:rag}, retrieval is always based on a similarity search. In this case, the similarity search is a hybrid search, meaning classical methods (BM25) and dense retrieval (embeddings) are combined. This hybrid search was supported by the python package weaviate, with accompanying vector database (see Figure \ref{fig:architecture_docker}). To combine the scores of two methods, the separate scores are first scaled such that the minimum is 0 and maximum is 1. Then they are combined with a weight: $\alpha \cdot \text{dense score} + (1-\alpha) \cdot \text{BM25 score}$. $\alpha$ has a default value of 0.7, focussing most on the embedder. By coincidence, this was found to be the optimum for the use case too. For BM25, some parameters are available to tune, but typically the defaults suffice. In this case the defaults were also used as implemented by weaviate.

The largest effort went into dense retriever. Not only are there typical parameters to tune, but since the rise of LLMs a whole new realm of tricks have allowed retrieval systems to push it to the limits. The discussion starts with the classical choices.

To start, we first need to find a good embedding model. A pragmatic approach is the only way not to get lost in the thousands of embedders, available on HuggingFace. To compare these embedders, a good benchmark is necessary that represents the Dutch, medical use case somewhat. Beyond MTEB, no reputable benchmarks were found to give a fair and insightful comparison between embedders. And needless to say, MTEB differs much from the intended use case. Surprisingly, reddit \cite{reddit2025embedding} turned out to be the most useful source of evaluations of models. This is likely, because Dutch is a niche language and a more professional comparison is not worthwhile. The best embedder is also found this way. The model is called BGE-M3 and it is a multilingual embedder. This property has proven to be the most important among all tested embedders. While a specialized model exists specifically for multilingual medical data, BioLORD-2023-M \cite{remy2024biolord}, we hypothesize that the BGE-M3 model performs better based on sheer size. BGE-M3 is 568M parameters large, while BioLORD-2023-M has only 109M parameters. Other factors could be at play too, but no deep analysis was done: whatever works works.

With this setup, we ran the first tests to analyze the performance and the weak spots of the current model. A recurring error was that the embedder did not match on chunks that were in the middle of a large explanation. A typical example would be a document where the title contains the important keywords such as "hemathology" or "oncology". As these documents were made for human readers, the rest of the full pdf assumes that the reader remembers that context. An example of a chunk with too little context inside is shown in Figure \ref{fig:out_of_context_zenya_doc}. So the chunks in se do not contain enough information to properly match with the query. As a rather quick solution, we appended the title to each chunk before being embedded. The structure is as follows: \verb|f'Titel: {title}\nTekst: {cleaned_text}|'. This simple method yielded approximately a 0.08 gain in MRR, which is an enormous boost.
TODO define MRR before

\begin{figure}[h]
    \centerline{\includegraphics[width=1\linewidth]{fig/out_of_context_zenya_doc.png}}
    \caption{Chunk of a pdf with the title ``Zorgpad invasieve blaastumor robotgeassisteerde cystectomie met neo-blaas''. The chunk does not contain enough information to place it into context. The title clearly adds some necessary context.}
    \label{fig:out_of_context_zenya_doc}
\end{figure}

The analysis was repeated to find the common causes of errors again. The most common source of mistakes was medical synonyms and abbreviations. This is not too surprising as the model was not trained for this. In fact, it is somewhat of a miracle that the model works this well without actually having a clue what most questions are about. Practically, this means that questions with uncommon terms, medical abbreviations or even abbreviations specific to the UZGent often did not match well with the intended chunks. This issue is quite fundamental and to the best of my, the only real solution is to fune tune the embedder. This would have taken much effort and in the end, the embedder does not stand central in this thesis. For this reason, the fine tuning is out of scope and it is future work. TODO add to future work. TODO future work double embedder?

\subsubsection{Ineffective retrieval techniques}
Of course, many methods were tried that did not lead to improvements. These methods are listed below. It is important to note that these methods do work in other settings and that it is specifically for this use case that the methods did not work. So either the method was not fit for this specific dataset, or it required larger models than what could be run locally on the UZGent servers. In other words, this chapter indicates what a future developer should not try again for this specific use case and nothing more.

One such method that did not work is to search chunks hierarchically. To see whether this was a technique worth considering, the implementation started with a very simple approach. First, the retriever fetches the chunks as usual, but then the scores are aggregated per document. Then the idea is to find the right document first before taking the best chunks out of that document. Now, there is no use in implementing it if the indicators are not promising. As an indicator, the MRR of the correct document was compared for the aggregating method versus just the top-k chunks' documents. The gains were negligible at first and with a later, higher quality validate set the scores even went down. So this idea was dropped. However, the principle of hierarchical search is still promising for future work, when more time and data is available. UZGent has metadata for each document (that were not available during the thesis), which could strengthen this method. For example, during practical usage multiple doctors and nurses gave the feedback that the retrieved chunks were very reasonable and contained the right information, but for another function. E.g. paediatrician got very relevant results, but for adults rather than kids. To help guide the model, the function of the user could be given to the retriever and the metadata could say for who which document is intended. This way either documents can be filtered or given extra score for being fit for the specific user asking the question. In conclusion, gathering this data and implementing hierarchical search is also future work.

The small to large method was also removed after implementation. The usual success of small to large is that the matched chunk is similar to the question, but not necessarily the answer. So certainly for small chunk sizes, the answer is to be expected before or after the chunk. And thus the solution is to retrieve a small chunk and then just expand, gathering also a piece before and after the retrieved chunk. During the process, this was first relevant, because the first tested models required small chunk sizes. Then, the final embedder was optimal for large chunk sizes, so this technique became less relevant. However, it later seemed that even for the large chunk sizes, the answer was often hidden before or after the chunk. This time, though, the chunks started crossing page boundaries and the used pdf parser made it harder to cross that border. While this is not the largest effort to fix, it was still not implemented. The most important reason was that the expected gains were for the generator rather than the retriever and at the time it already became clear that there is probably no future for the generator (as discussed in Section \ref{Ethical considerations}).

Another method that failed in this particular use case was reranking. The reranking method starts as usual: the query is embedded and a similarity search finds the top-k most relevant chunks. Then the reranker model compares the query to each single chunk to give a similarity score. So the original order is dropped and replaced by the order that the reranker finds most relevant. The reason that this should work better, is that classical retrieval has an intermediary step of an embedding which implicitly encodes similarity. Learning the similarity directly should be an easier task and thus these models should perform better. The bge-reranker-v2-m3 was tried, as the embedding model of the same publisher performed so well, but the MRR went down when reranking. Further analysis was beyond the scope of this thesis and thus the idea of reranking was also left out.

While rechunking seemed promising too, it did not deliver improved scores. Rechunking works as follows: first the classical retrieval is performed and then the retrieved chunk(s) are split again in even narrower parts. These new parts are again scored based on their similarity with the query to find the best match with a much smaller granularity. This works well in cases where the goal is to find one small piece of text. However, this seemed to conflict with the goals of the application. Even though the answer to a dummy question such as "How much Vancomycin do I give to a 46-years old patient with a viral infection?" could be as simple and short as "15 mg/kg", it might still be a very bad answer. One problem could be that this answer came from the document "Vancomycin for patients younger than 18". Even common metrics such as ragas' faithfulness would praise this answer as it was grounded in the retrieved chunks (though it was the wrong chunk and wrong document). To further dive into the possible problems, the question was a trick question. (TODO source \url{https://en.wikipedia.org/wiki/Vancomycin}). The internet says Vancomycin is for bacterial infections rather than viral infections. Though this is a hypothetical example, it was also found that in real cases the critical context was found far above of below the retrieved chunk. So the perfect answer of "Do not use Vancomycin for viral infections" has very low chances of actually being the output of the chatbot. To solve the problem of missing context, one could just try to give the whole document to the generator. This is not the solution, because the missing context is often times implicit as the writer of the document thought of it as common sense, which the generator does not have. Specifically in this case, the limitations of a 9B model also became clear when too much context was given.

HyDE TODO source also could not live up to the expectations. The reasoning behind the technique is that the question and the answer in the documents can be semantically very different. To benifit from this observation, TODO et al tried letting the LLM answer the question first, making a hypothetical answer. Then the similarity of the chunks is calculated with the hypothetical answer rather than the question itself. Finally, the similarity score of (multiple attempts of) this method and the original score are combined into one final score. This principle seemed perfect for this use case, as questions are typically very different from their answers. However, the model had a lack of understanding in the Dutch medical domain. This meant the envisioned increase in semantic similarity could not compensate for the noise the generator added.

\subsection{Generation}
After the retriever has found a list of possibly relevant chunks, the generator uses that information to generate an answer. To optimize a generator, there are broadly speaking two common options: making the prompt better and making the model better. The latter option is usually work-intensive and resource-intensive, making it out of scope for this thesis. By consequence, the following paragraphs will discuss only prompt engineering techniques used in the application.

TODO \url{https://arxiv.org/pdf/2005.14165v4} for the following paragraphs.

The first technique is to properly separate pieces of information. The pragmatic approach was to use the first separation technique that the generator seemed to handle well. This turned out to be the following style:
\begin{verbatim}
[begin context]
Jij bent een dokter. Jij kent alle medische procedures van het UZGent.
[einde context]
[begin documenten]
{context}
[einde documenten]
[begin vraag]
{query}
[einde vraag]
[begin antwoord]
\end{verbatim}
This is of course the prompt for the G in RAG. As stated before, the generator was also used in other settings, but the same style with brackets and "begin" and "einde" was kept. As a sidenote, the context of "You know all medical procedures of the UZGent" might seem weird, motivating the generator to answer possibly more often than it should. However, without this part the model seemed to be fine tuned to say it cannot answer medical questions. So it is a little hack to get an answer at all.

The second technique is few-shot prompting TODO source. This uses in-context learning to show the model what typical input is and the corresponding model answer. At the time of development, the prefered answer of the full pipeline was still unclear, so few-shot prompting was not used. But for other techniques, such as HyDE, it was used to get improved results. 

\subsection{Postprocessing}
The last part of the pipeline is to make the output presentable for the end user. The generated answer can be shown directly, but giving the sources in plain text is not that user friendly. To this end, the plain text was searched again in the original document (pdf) and highlighted. And as a nice addition, the pdf is automatically scrolled to the highlighted part.

\section{Application diagrams}
Significant effort was put into engaging the UZGent with the thesis and the understanding behind it. This was met with great enthusiasm from their side. That is why the following diagrams were made to clarify what was going on in the background. The first diagram in Figure \ref{fig:architecture_docker} explains the whole application in high level components. Zooming in on the interesting part, Figure \ref{fig:architecture_python} shows how the API works internally.

\begin{figure}[H]
    \captionsetup{justification=centering}
    \centerline{\includegraphics[width=1\linewidth]{fig/Architecture Docker.png}}
    \caption{A high level diagram that lists the different docker containers. While a simple diagram now, this used to be different. Significant effort was put into containerizing each component, making them as independent as possible and building those efficiently to reduce build time. The orchestrator is simply a github repository that contains a docker compose file and some necessary configurations. This docker compose will then spin up the docker containers. Most of them are straight from an image, but some of them are built will local code. The RAG container is built from local python code and it runs the API. The API's input is a question and it returns the answer with corresponding chunks. The website is also built from local code and it is a typical chat interface that forwards questions to the API. The reverse proxy uses the well known nginx in docker to allow https connections to the website. The vector database is also a default weaviate container, just like the database is a default postgres container. Finally, vLLM also supports a containerized version making it very easy to host LLMs in a scalable way.}
    \label{fig:architecture_docker}
\end{figure}

\begin{figure}[H]
    \captionsetup{justification=centering}
    \centerline{\includegraphics[width=1\linewidth]{fig/Architecture Python.png}}
    \caption{A high level diagram of the class dependencies in python of the RAG part in the API.}
    \label{fig:architecture_python}
\end{figure}

\section{Test results and feedback}
This section discusses both the test results and the informal feedback gathered during the sessions.

Before going into the test results, some caveats must be pointed out. First of all, this test data is important for future use at the UZGent, as they want to use it to evaluate all possible competitors that claim they can do better than the current system, Zenya. This means a validate set is necessary to hand over to the competitor and a test set to keep for themselves. As the writer of this thesis is both the person gathering the data, making the split and also testing his system, this is something of a violation against best practices. Thus, the UZGent will have to trust the writer not to make any decisions based on the test set.

The next problem, is that the test set turned out to be quite unclean. For example 20\% of the ground truth documents were derived from the context. While this would work if every tester followed the instructions prefectly, testers remain human and make mistakes. Thus sometimes this derived ground truth is not actually right. Additionally, 8\% of the questions were left without any clue what the ground truth would be. Realistically, these questions should be revised by an unbiased person at the UZGent itself, but this manual labor is hard to schedule in the short time span of a thesis. (TODO is this fixed?)

That being said, the results are a mix of multiple interpretations that try to keep these issues into account. First, the direct results of the full test session are given. These yield the most pessimistic view, because of the noise discussed before and there is no bias towards the writer's opinion at all. Figure \ref{fig:found_rate} shows the most high level question: did the user find the answer or not? Then, Figure \ref{fig:liked_docs_per_convo} analyzes how many unique liked documents a conversation had. A conversation is defined as a sequence of messages to the chatbot until the healthcare professional is comfortable with the answer and they would use the information in real life. On a finer granularity, Figure \ref{fig:liked_docs} looks at the number of liked documents per question.

\begin{figure}[H]
    \captionsetup{justification=centering}
    \centerline{\includegraphics[width=0.9\linewidth]{fig/RAG_found_plot.png}}
    \caption{The distribution of all conversations, indicating whether the answer was found or not. And this both for the RAG system and the old system, Zenya. The first thing that becomes clear is that the answer is almost always found in Zenya. The discussion about why this is biased is given in the paragraph about the informal feedback. The second conclusion is that 54\% of the questions were properly answered by the RAG system. This is not exactly a great result, but let us consider all metrics before drawing conclusions.}
    \label{fig:found_rate}
\end{figure}

\begin{figure}[H]
    \captionsetup{justification=centering}
    \centerline{\includegraphics[width=0.7\linewidth]{fig/RAG_nr_correct_in_conversation.png}}
    \caption{Distribution of the number of unique correct documents found in a whole conversation. The most important insight in the writer's opinion is that about 35\% of the conversations had no right document found, yet 46\% of the conversations were labeled as "answer not found". This further indicates the need for cleaning of the seemingly inconsistent dataset. While many other conclusions could be drawn here, the discussion is limited to the essentials. This is because focusing on the details could be focusing on the noise as described in the caveats.}
    \label{fig:liked_docs_per_convo}
\end{figure}

\begin{figure}[H]
    \captionsetup{justification=centering}
    \centerline{\includegraphics[width=0.7\linewidth]{fig/RAG_nr_top_3.png}}
    \caption{Distribution of the number of unique correct documents found per individual message. The first interesting feature is that sometimes multiple documents are labeled as good. This information is invaluable to the UZGent, as they expected one document to always suffice. This could show the existence duplicate information across documents or it could indicate complexer questions have their answer scattered across documents. A detailed investigation of this is left to the experts of the Zenya data at the UZGent. Next to that, it can be seen that 48\% of the questions did not get a relevant document returned. This number does not really impress, though it is the pessimistic view of course.}
    \label{fig:liked_docs}
\end{figure}

While the direct results are the most unfiltered, multiple red flags indicate that the statistics are not extremely reliable. For this reason, the writer filtered out the noise as follows, trying to remain completely unbiased in the process. All messages without any possible way of deducting the right document were left out, as approved by the UZGent. TODO rewrite after toms input

\begin{figure}[H]
    \captionsetup{justification=centering}
    %\centerline{\includegraphics[width=0.7\linewidth]{fig/TODO add picture}}
    \caption{Distribution of the rank of the first correct document. This visualization shows very well how the most queries find the right document in the earlier ranks. When a document was not in the top 10, the chances clearly drop that the document is found at all. An important note to make is that a document not found is displayed as the maximal rank: 50 (TODO check number before). With this information, one can see that about 15\% (TODO check number before) of the documents was totally not found.}
    \label{fig:chunk_rank_density}
\end{figure}

\begin{figure}[H]
    \captionsetup{justification=centering}
    %\centerline{\includegraphics[width=0.7\linewidth]{fig/TODO add picture}}
    \caption{Cumulative distribution of the rank of the first correct document. This representation shows better what portion was already found by rank x. This shows the top 3 documents already have the right one TODO \% of the time. For the top 10 documents, that is TODO \%. Then only TODO \% is still found in the next 40 documents and as said before, TODO \% are not found at all.}
    \label{fig:chunk_rank_cumulative_distribution}
\end{figure}

The informal feedback provided great nuance to the direct feedback. The first and most important conclusion by most testers was that this system is not ready for production yet. As they were refering to the whole RAG system, this was a logical conclusion, given the large number of hallucinations. However, when asked about the retrieval only, the testers were more positive. Some were already convinced about the new model, while other still saw much work to be done. As a trend, the more niche testers were less convinced than the other ones. Interestingly, they said themselves that this problem could be very easily fixed by adding user information to the query. This means that a paediatrician should only get documents about children, or at least get them first. This information was not available, so it could not be implemented during the thesis. However, this is very important to be aware of for any party making retrievers for the UZGent.

One particular anecdote about the informal feedback really stood out: a nurse said that they are usually looking for five minutes in Zenya before finding the right document, if it is found at all. While this was the most extreme example, the sentiment of the other testers seemed to be similar. They claimed that one needs to know what they are looking for exactly, before searching or else one will never find the right document. This is of course the opposite of how a good retrieval system should work. The main conclusion out of this is that the results in Figure \ref{fig:found_rate} are to be taken with a grain of salt. For a fair comparison of both systems, one should choose testers who do not know either system yet. This will probably shift the results towards "not found" in Zenya, possibly even indicating the RAG system as the winner. Such tests are future work though.

Another type of feedback was implicit. Some testers asked how to pose their questions or just struggled finding a good question by themselves. This means asking a logical, natural language question is not as easy as first thought. Now, it must be said that the testers were biased. As they are Zenya experts that were trained for years to use only keywords, giving a natural language question to a computer just felt off for them.

\section{Future work at the UZGent and on the RAG system}
In this section, a list of ideas is given to the UZGent, should they be interested to further develop the RAG system.

The first step to do proper machine learning in general, is to have a high quality validate and test set. This is important, not only for further development, but also to compare other systems. For this reason, this is deemed to be the most important work, that needs to happen first.

The next steps are ordered according to expected impact on the scores. As said before, adding metadata to the retriever's input could provide context that is necessary to retrieve the right document. Both metadata about the documents (which exists, but was not available during the thesis) and metadata about the user, which can be used to filter, but also to help the AI components. Another extremely relevant improvement is to find a better embedder. While the currently used embedder is with reasonable certainty the best for the use case today, new great embedders pop up on HuggingFace every now and then. And as the difference between the best and second best embedder was immense, a new model might as well make such a large jump in the score.

As final recommendations for the UZGent the writer would suggest the following. Drop the generative model. This will reduce the computational cost, the energy consumption and most importantly and it does not improve the retriever anyway. With that being said, the server requirements drop significantly and for the low load, the retriever might even run on CPU. In other words, there are only wins with this choice. The next recommendation is to implement any more modern retriever; it should not even be the one from this thesis, as there is much low-hanging fruit to be picked. And of course as a last recommendation, the future work listed above should be performed.

\section{Ethical considerations}
\label{Ethical considerations}
While the critical domain of healthcare brings many ethical considerations, this section will focus on one specifically: the hallucinations of the generator. For clear context: the demand for a RAG application came from the UZGent itself, before being implemented for this thesis. However, in the presented use case, we cannot afford critical mistakes with odds one to a million. As a typical generator hallucinates with much higher odds, the use case was invalid to begin with. For this reason, it is proposed only to work further with the retriever and to be very wary about any application that uses a generator directly to hand critical information to a healthcare professional.

To make this suggestion more hands-on, some examples are given of good versus risky applications. A retriever using HyDE is good. There, the generator is used to augment the retriever and the final output is an existing document. As the documents are trustworthy, no wrong results are possible. Worst case, the wrong document is returned, the doctor sees this and then chooses another document. As a bad example, consider the RAG use case. Some documents are retrieved, then the generator makes an answer with them. The generator is the last step that gives possibly critical information. The generator could say anything wrong and if that were to be executed, it could have lethal effects. Now, one might say "make sure the answer is grounded in the context". That does not matter if it is looking at the wrong context. The best example is two documents, one for people of age up to 15 years old and another for people older than 15. These documents are semantically very close and contain nearly the same information making confusion very likely. Yet the doses could differ significantly and groundedness would not know it is looking at the wrong document. Another critical person could say "but the healthcare professionals" will notice, they have to check the sources for themselves. First of all, people become lazy and stop checking the sources, that is just a fact. And even then, when it is required to find the answer in the document every time to verify, why is the answer even generated? All "solutions" just seem like methods to boost confidence in the system, because mistakes are less likely (or at least seem less likely), yet more confidence just means more chance that the day a mistake inevitably occurs, it is trusted.

