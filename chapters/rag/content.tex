
\chapter{RAG}


\section{RAG}
A modern RAG pipeline consists of many more stages than a simple retrieve and then generate. The common steps are: TODO, each of which can take many different forms depending on the application.

\subsection{Pre-Retrieval}
In the pre-refrieval, the input of the user is first processed. As a normal user is not an experienced prompt engineer, the questions users ask are more often than not incomplete, ambiguous, ... To better interpret the question, three different methods have emerged: splitting the question, abstracting the question and adding a hypothetical answer.
\subsubsection{Splitting the question}
A question often consists of multiple subquestions. Demonstrate-Search-Predict and IRCoT try to leverage the chain of thought strategy, where the question is reduced to a simpler question iteratively. Laster, each question will then be answered with its own relevant retrieved documents. More parallel methods like ToC and RAG-Fusion break down the question recursively into multiple questions. The difference with CoT is that there are now multiple branches.
\subsubsection{Abstracting the question}
Step-Back Prompting tries to make the retrieval easier by first abstracting the question. The idea is that highly specific questions with uncommon words can mess with the embeddings, preventing the retriever to find semantically similar, relevant documents.
\subsubsection{Adding a hypothetical answer}
HyDE builds a hypothetical answer to the question. This hypothetical answer can contain hallucinations, but that is not a problem. The point is that the hypothetical answer should be semantically closer to the real answer than the question is.

\subsection{Retriever}
A good retriever is a key component of a good RAG system. If the context is filled with irrelevant information, this can make the answer even worse. Most often a RAG retriever works with a hybrid model. This combines the advantages of a lexical search and a semantic search. Lexical search works on keywords and is good to find literal matches between the question and documents. This is complemented with dense retrieval. Dense retrievers search semantically, by comparing the embeddings of the documents and the question. There are many strong retrievers for general use cases, such as Splade, Contriever, GTR, ... These could server as a basis for fine-tuning on a specific domain, such as the medical domain for this thesis. However, this work has already been done by BioLord (TODO gebruik ik dit echt?). BioLord used contrastive learning to fine-tune an existing embedding model to perform better on medical data.

\subsection{Post-retrieval}
Retrieval usually has a rather course granularity. This means that the retrieved documents might contain the relevant information, but also much noise around it. To reduce this noise, post-retrieval tries to filter the context to only the relevant parts. This is a use case of prompt compression. Also, a reranker will reorder the retrieved documents to place the most important parts at the start. This is because an LM takes the first sentences more into account. This effect is known as the lost-in-the-middle effect.
 
\subsubsection{Prompt compression}
Prompt compression intends to make the prompt, the input of the LM, shorter, while still maintaining the meaning behind the prompt. Prompt compression usually gets subdivided into three categories: extractive compression, abstractive compression and token pruning.
\subsubsection{Why prompt compression}
There are multiple reasons to keep the length of the input for a LM limited. Firstly, each model has a limited context window and it is impossible for the LM process the full input when the input gets longer. Secondly, longer input can degrade the quality of the generation. This is because typically LMs are trained with fixed length inputs, much shorter than the context window. So any input longer than that is actually out of distribution. Lastly, longer input increases the necessary computation, which leads to longer latency and more energy consumption.
\subsubsection{Extractive compression}
Extractive compression, the type of compression that currently performs best, is based on extracting relevant parts from the original text. These parts can have a granularity of phrases, sentences or passages TODO? In RAG systems, this extraction can be based on the semantic similarity between a part of the context and the input prompt. 

Below, the most interesting methods for this thesis are described: CPC, RECOMP and Selective Context, ordered by relevance. CPC uses contrastive learning to fine-tune an embedder, such that it can find similarity between context sentences and the question asked. The authors noticed that embedders are good at finding direct answers to a question, but not components of answers, so they trained the embedder to learn similarity between questions and parts of answers. This method seems to work best at finding relevant sentences. Only the relevant sentences are then kept and the irrelevant ones are cut out. RECOMP has a similar method of contrastive learning, but they get their positive samples from sentences that bring the perplexity of the ground truth answer down most. Selective Context uses a whole different strategy, where they combine the token-level perplexities to get a phrase level perplexity. The lowest perplexity phrases are then cut out.
\subsubsection{Abstractive compression}
In abstractive compression, the prompt is summarized by a LM. This summary can contain new tokens that were not in the original prompt, unlike the other techniques.
\subsubsection{Token pruning}
Token pruning tries to use the fact that a LM does not read in the same way a human does. By cutting away tokens that were "obvious" to a LM anyway, the compressor can try to condense information by cutting only the obvious tokens and leaving the surprising (read high perplexity) tokens as is. This method has a known issue, where the final generation copies literal parts of the prompt and thus the LM also outputs rather incoherent text. In token compression, LLMLingua laid the foundation for both LLMLingua 2 and LongLLMLingua. They are all based on the same principle of using a smaller model's perplexity to estimate the relevance of a token. Then the low perplexity tokens are cut iteratively, recalculating perplexity every time. A budget controller decides what perplexity is too low.

\subsubsection{Reranker}
A reranker is made to order chunks according to relevance. While a straightforward method is to us an embedding model again, other recent techniques can get better scores. One method that stands out is QLM, where an LLM is used to rerank the chunks. In this context, such a method is possible, because there has already been strong filtering on all chunks and this would not scale to a full retriever. However, the quality of such reranker seems to be better than retriever-models. This usually comes at the cost of a larger reranker model though.


\subsection{Generator energy optimization}
\subsubsection{Speculative decoding}
Speculative decoding tries to reduce the number of LM forward passes. Less forward passes in the LM mean not only less latency, but also more energy efficiency. The key idea to speculative decoding is that when the output of an LM is known, the LM can verify all tokens in that output in parallel. However, to know the output, we need to use the LM. This creates a chicken or the egg problem, but it can be solved by guessing. First we guess the output of the LM, not only for the next token, but also for further tokens. With that guess, all tokens are checked in parallel. Needless to say, the open question is: how is a good guess made without calling the LM itself and how much does that estimation cost itself.

The most relevant techniques for this thesis are EAGLE 2, TriForce and Inference with Reference. EAGLE 2 makes not one prediction, but a tree of predictions. All of these branches will then be checked in parallel. Such a branch is made autoregressively, much like a normal LM. However, in EAGLE they trained a smaller model to use the features of the large model and the output to then predict the features of the next token and the output. This way, the smaller model uses some of the internal knowledge of the large model in subsequent predictions. This method seems to work well, as it performs best of all speculative decoders up until oct 29 TODO. TriForce uses a more standard approach of letting a smaller model draft the next tokens and then using the large model to verify. However, it does so hierarchically, with two draft layers. Lastly, Inference with Reference notes that in situations where copying context is common (such as RAG applications), using parts of the context as a next guess can also work well.
\subsubsection{Copy is all you need}
CoG, instead of generating tokens only, also generate passages of the context.

\subsubsection{AutoMix}
SLM as generator, SLM as judge + POMPD + reroute to LLM if necessary

Post-generation
Source highlighting
	Source highlighting via constrained decoding (ask LLM which source it used)
	https://huggingface.co/learn/cookbook/en/structured\_generation
	Finetuning SLM to quote sources
https://community.openai.com/t/source-document-chunk-identification-and-highlighting-for-rag-usecase/883302/2
Post-hoc semantic search?
Loop
FLARE
	Retrieve when generated sentence has low probability tokens
Self-RAG
	Generate retrieval tokens (needs finetuning)
Adaptive RAG
Train SLM to classify single-retrieval question vs multi-retrieval (for efficiency rather than quality)