@article{EAGLE-2,
  author={Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang},
  year={2024},
  title={Eagle-2: Faster inference of language models with dynamic draft trees},
  journal={arXiv preprint arXiv:2406.16858},
}

@article{li2024eagle,
  title={Eagle-2: Faster inference of language models with dynamic draft trees},
  author={Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang},
  journal={arXiv preprint arXiv:2406.16858},
  year={2024}
}

@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{shareGPT,
  title={ShareGPT Dataset},
  author={Ryoko AI},
  url={https://huggingface.co/datasets/RyokoAI/ShareGPT52K},
}

@article{li2024eagle1,
  title={Eagle: Speculative sampling requires rethinking feature uncertainty},
  author={Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang},
  journal={arXiv preprint arXiv:2401.15077},
  year={2024}
}

@article{cai2024medusa,
  title={Medusa: Simple llm inference acceleration framework with multiple decoding heads},
  author={Cai, Tianle and Li, Yuhong and Geng, Zhengyang and Peng, Hongwu and Lee, Jason D and Chen, Deming and Dao, Tri},
  journal={arXiv preprint arXiv:2401.10774},
  year={2024}
}

@inproceedings{leviathan2023fast,
  title={Fast inference from transformers via speculative decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  booktitle={International Conference on Machine Learning},
  pages={19274--19286},
  year={2023},
  organization={PMLR}
}

@article{he2023rest,
  title={Rest: Retrieval-based speculative decoding},
  author={He, Zhenyu and Zhong, Zexuan and Cai, Tianle and Lee, Jason D and He, Di},
  journal={arXiv preprint arXiv:2311.08252},
  year={2023}
}

@article{qin2024optimized,
  title={Optimized Multi-Token Joint Decoding with Auxiliary Model for LLM Inference},
  author={Qin, Zongyue and Hu, Ziniu and He, Zifan and Prakriya, Neha and Cong, Jason and Sun, Yizhou},
  journal={arXiv preprint arXiv:2407.09722},
  year={2024}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

@article{khattab2022demonstrate,
  title={Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp},
  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2212.14024},
  year={2022}
}

@article{trivedi2022interleaving,
  title={Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions},
  author={Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2212.10509},
  year={2022}
}

@inproceedings{kim2023tree,
  title={Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models},
  author={Kim, Gangwoo and Kim, Sungdong and Jeon, Byeongguk and Park, Joonsuk and Kang, Jaewoo},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={996--1009},
  year={2023}
}

@article{rackauckas2024ragfusion,
  title={Rag-fusion: a new take on retrieval-augmented generation},
  author={Rackauckas, Zackary},
  journal={arXiv preprint arXiv:2402.03367},
  year={2024}
}

@article{zheng2023takeastepback,
  title={Take a step back: Evoking reasoning via abstraction in large language models},
  author={Zheng, Huaixiu Steven and Mishra, Swaroop and Chen, Xinyun and Cheng, Heng-Tze and Chi, Ed H and Le, Quoc V and Zhou, Denny},
  journal={arXiv preprint arXiv:2310.06117},
  year={2023}
}

@inproceedings{gao2023precisehyde,
  title={Precise zero-shot dense retrieval without relevance labels},
  author={Gao, Luyu and Ma, Xueguang and Lin, Jimmy and Callan, Jamie},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1762--1777},
  year={2023}
}

@inproceedings{formal2021splade,
  title={SPLADE: Sparse lexical and expansion model for first stage ranking},
  author={Formal, Thibault and Piwowarski, Benjamin and Clinchant, St{\'e}phane},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2288--2292},
  year={2021}
}

@article{formal2021spladev2,
  title={SPLADE v2: Sparse lexical and expansion model for information retrieval},
  author={Formal, Thibault and Lassance, Carlos and Piwowarski, Benjamin and Clinchant, St{\'e}phane},
  journal={arXiv preprint arXiv:2109.10086},
  year={2021}
}

@article{lassance2024spladev3,
  title={SPLADE-v3: New baselines for SPLADE},
  author={Lassance, Carlos and D{\'e}jean, Herv{\'e} and Formal, Thibault and Clinchant, St{\'e}phane},
  journal={arXiv preprint arXiv:2403.06789},
  year={2024}
}

@article{izacard2021unsupervisedcontriever,
  title={Unsupervised dense information retrieval with contrastive learning},
  author={Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
  journal={arXiv preprint arXiv:2112.09118},
  year={2021}
}

@article{ni2021largegtr,
  title={Large dual encoders are generalizable retrievers},
  author={Ni, Jianmo and Qu, Chen and Lu, Jing and Dai, Zhuyun and {\'A}brego, Gustavo Hern{\'a}ndez and Ma, Ji and Zhao, Vincent Y and Luan, Yi and Hall, Keith B and Chang, Ming-Wei and others},
  journal={arXiv preprint arXiv:2112.07899},
  year={2021}
}

@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}