@article{EAGLE-2,
  author={Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang},
  year={2024},
  title={Eagle-2: Faster inference of language models with dynamic draft trees},
  journal={arXiv preprint arXiv:2406.16858},
}

@article{li2024eagle,
  title={Eagle-2: Faster inference of language models with dynamic draft trees},
  author={Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang},
  journal={arXiv preprint arXiv:2406.16858},
  year={2024}
}

@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{shareGPT,
  title={ShareGPT Dataset},
  author={Ryoko AI},
  url={https://huggingface.co/datasets/RyokoAI/ShareGPT52K},
}

@article{li2024eagle1,
  title={Eagle: Speculative sampling requires rethinking feature uncertainty},
  author={Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang},
  journal={arXiv preprint arXiv:2401.15077},
  year={2024}
}

@article{cai2024medusa,
  title={Medusa: Simple llm inference acceleration framework with multiple decoding heads},
  author={Cai, Tianle and Li, Yuhong and Geng, Zhengyang and Peng, Hongwu and Lee, Jason D and Chen, Deming and Dao, Tri},
  journal={arXiv preprint arXiv:2401.10774},
  year={2024}
}

@inproceedings{leviathan2023fast,
  title={Fast inference from transformers via speculative decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  booktitle={International Conference on Machine Learning},
  pages={19274--19286},
  year={2023},
  organization={PMLR}
}

@article{he2023rest,
  title={Rest: Retrieval-based speculative decoding},
  author={He, Zhenyu and Zhong, Zexuan and Cai, Tianle and Lee, Jason D and He, Di},
  journal={arXiv preprint arXiv:2311.08252},
  year={2023}
}

@article{qin2024optimized,
  title={Optimized Multi-Token Joint Decoding with Auxiliary Model for LLM Inference},
  author={Qin, Zongyue and Hu, Ziniu and He, Zifan and Prakriya, Neha and Cong, Jason and Sun, Yizhou},
  journal={arXiv preprint arXiv:2407.09722},
  year={2024}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

@article{khattab2022demonstrate,
  title={Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp},
  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2212.14024},
  year={2022}
}

@article{trivedi2022interleaving,
  title={Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions},
  author={Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2212.10509},
  year={2022}
}

@inproceedings{kim2023tree,
  title={Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models},
  author={Kim, Gangwoo and Kim, Sungdong and Jeon, Byeongguk and Park, Joonsuk and Kang, Jaewoo},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={996--1009},
  year={2023}
}

@article{rackauckas2024ragfusion,
  title={Rag-fusion: a new take on retrieval-augmented generation},
  author={Rackauckas, Zackary},
  journal={arXiv preprint arXiv:2402.03367},
  year={2024}
}

@article{zheng2023takeastepback,
  title={Take a step back: Evoking reasoning via abstraction in large language models},
  author={Zheng, Huaixiu Steven and Mishra, Swaroop and Chen, Xinyun and Cheng, Heng-Tze and Chi, Ed H and Le, Quoc V and Zhou, Denny},
  journal={arXiv preprint arXiv:2310.06117},
  year={2023}
}

@inproceedings{gao2023precisehyde,
  title={Precise zero-shot dense retrieval without relevance labels},
  author={Gao, Luyu and Ma, Xueguang and Lin, Jimmy and Callan, Jamie},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1762--1777},
  year={2023}
}

@inproceedings{formal2021splade,
  title={SPLADE: Sparse lexical and expansion model for first stage ranking},
  author={Formal, Thibault and Piwowarski, Benjamin and Clinchant, St{\'e}phane},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2288--2292},
  year={2021}
}

@article{formal2021spladev2,
  title={SPLADE v2: Sparse lexical and expansion model for information retrieval},
  author={Formal, Thibault and Lassance, Carlos and Piwowarski, Benjamin and Clinchant, St{\'e}phane},
  journal={arXiv preprint arXiv:2109.10086},
  year={2021}
}

@article{lassance2024spladev3,
  title={SPLADE-v3: New baselines for SPLADE},
  author={Lassance, Carlos and D{\'e}jean, Herv{\'e} and Formal, Thibault and Clinchant, St{\'e}phane},
  journal={arXiv preprint arXiv:2403.06789},
  year={2024}
}

@article{izacard2021unsupervisedcontriever,
  title={Unsupervised dense information retrieval with contrastive learning},
  author={Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
  journal={arXiv preprint arXiv:2112.09118},
  year={2021}
}

@article{ni2021largegtr,
  title={Large dual encoders are generalizable retrievers},
  author={Ni, Jianmo and Qu, Chen and Lu, Jing and Dai, Zhuyun and {\'A}brego, Gustavo Hern{\'a}ndez and Ma, Ji and Zhao, Vincent Y and Luan, Yi and Hall, Keith B and Chang, Ming-Wei and others},
  journal={arXiv preprint arXiv:2112.07899},
  year={2021}
}

@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{chen2024bge,
  title={Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation},
  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},
  journal={arXiv preprint arXiv:2402.03216},
  year={2024}
}

@article{zhuang2023qlm,
  title={Open-source large language models are strong zero-shot query likelihood models for document ranking},
  author={Zhuang, Shengyao and Liu, Bing and Koopman, Bevan and Zuccon, Guido},
  journal={arXiv preprint arXiv:2310.13243},
  year={2023}
}

@misc{reddit2025embedding,
  author = {{u/fictioninquire}},
  title = {Best open source embedding models for EU languages},
  year = {2024},
  url = {https://www.reddit.com/r/LocalLLaMA/comments/1chqkph/best_open_source_embedding_models_for_eu_languages/},
  note = {Reddit thread}
}

@article{remy2024biolord,
  title={BioLORD-2023: semantic textual representations fusing large language models and clinical knowledge graph insights},
  author={Remy, Fran{\c{c}}ois and Demuynck, Kris and Demeester, Thomas},
  journal={Journal of the American Medical Informatics Association},
  volume={31},
  number={9},
  pages={1844--1855},
  year={2024},
  publisher={Oxford University Press}
}

@article{levy2024sameinputlength,
  title={Same task, more tokens: the impact of input length on the reasoning performance of large language models},
  author={Levy, Mosh and Jacoby, Alon and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2402.14848},
  year={2024}
}

@article{kim2023arithmeticintensityllm,
  title={Full stack optimization of transformer inference: a survey},
  author={Kim, Sehoon and Hooper, Coleman and Wattanawong, Thanakul and Kang, Minwoo and Yan, Ruohan and Genc, Hasan and Dinh, Grace and Huang, Qijing and Keutzer, Kurt and Mahoney, Michael W and others},
  journal={arXiv preprint arXiv:2302.14017},
  year={2023}
}

@inproceedings{liskavets2025cpc,
  title={Prompt compression with context-aware sentence encoding for fast and improved llm inference},
  author={Liskavets, Barys and Ushakov, Maxim and Roy, Shuvendu and Klibanov, Mark and Etemad, Ali and Luke, Shane K},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={23},
  pages={24595--24604},
  year={2025}
}

@article{xu2023recomp,
  title={Recomp: Improving retrieval-augmented lms with compression and selective augmentation},
  author={Xu, Fangyuan and Shi, Weijia and Choi, Eunsol},
  journal={arXiv preprint arXiv:2310.04408},
  year={2023}
}

@article{li2023selectivecontext,
  title={Compressing context to enhance inference efficiency of large language models},
  author={Li, Yucheng and Dong, Bo and Lin, Chenghua and Guerin, Frank},
  journal={arXiv preprint arXiv:2310.06201},
  year={2023}
}

@article{jiang2023llmlingua,
  title={Llmlingua: Compressing prompts for accelerated inference of large language models},
  author={Jiang, Huiqiang and Wu, Qianhui and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
  journal={arXiv preprint arXiv:2310.05736},
  year={2023}
}

@article{pan2024llmlingua2,
  title={Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression},
  author={Pan, Zhuoshi and Wu, Qianhui and Jiang, Huiqiang and Xia, Menglin and Luo, Xufang and Zhang, Jue and Lin, Qingwei and R{\"u}hle, Victor and Yang, Yuqing and Lin, Chin-Yew and others},
  journal={arXiv preprint arXiv:2403.12968},
  year={2024}
}

@article{jiang2023longllmlingua,
  title={Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression},
  author={Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Li, Dongsheng and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
  journal={arXiv preprint arXiv:2310.06839},
  year={2023}
}

@inproceedings{es2024ragas,
  title={Ragas: Automated evaluation of retrieval augmented generation},
  author={Es, Shahul and James, Jithin and Anke, Luis Espinosa and Schockaert, Steven},
  booktitle={Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations},
  pages={150--158},
  year={2024}
}

@misc{vancomycin_wikipedia,
  title={Vancomycin},
  url={https://en.wikipedia.org/wiki/Vancomycin},
  urldate={2025-04-30}, 
}

@misc{eiffel_wikipedia,
  title={Eiffel Tower},
  url={https://en.wikipedia.org/wiki/Eiffel_Tower},
  urldate={2025-04-30}, 
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{kwiatkowski2019natural,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{nallapati2016abstractive,
  title={Abstractive text summarization using sequence-to-sequence rnns and beyond},
  author={Nallapati, Ramesh and Zhou, Bowen and Gulcehre, Caglar and Xiang, Bing and others},
  journal={arXiv preprint arXiv:1602.06023},
  year={2016}
}

@article{yang2023inference,
  title={Inference with reference: Lossless acceleration of large language models},
  author={Yang, Nan and Ge, Tao and Wang, Liang and Jiao, Binxing and Jiang, Daxin and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2304.04487},
  year={2023}
}

@article{chen2023unleashing,
  title={Unleashing the potential of prompt engineering in large language models: a comprehensive review},
  author={Chen, Banghao and Zhang, Zhaofeng and Langren{\'e}, Nicolas and Zhu, Shengxin},
  journal={arXiv preprint arXiv:2310.14735},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{white2023prompt,
  title={A prompt pattern catalog to enhance prompt engineering with chatgpt},
  author={White, Jules and Fu, Quchen and Hays, Sam and Sandborn, Michael and Olea, Carlos and Gilbert, Henry and Elnashar, Ashraf and Spencer-Smith, Jesse and Schmidt, Douglas C},
  journal={arXiv preprint arXiv:2302.11382},
  year={2023}
}

@article{xu2023parameter,
  title={Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment},
  author={Xu, Lingling and Xie, Haoran and Qin, Si-Zhao Joe and Tao, Xiaohui and Wang, Fu Lee},
  journal={arXiv preprint arXiv:2312.12148},
  year={2023}
}

@article{han2024parameter,
  title={Parameter-efficient fine-tuning for large models: A comprehensive survey},
  author={Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Jeff and Zhang, Sai Qian},
  journal={arXiv preprint arXiv:2403.14608},
  year={2024}
}

@inproceedings{fan2024survey,
  title={A survey on rag meeting llms: Towards retrieval-augmented large language models},
  author={Fan, Wenqi and Ding, Yujuan and Ning, Liangbo and Wang, Shijie and Li, Hengyun and Yin, Dawei and Chua, Tat-Seng and Li, Qing},
  booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={6491--6501},
  year={2024}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  volume={1},
  number={2},
  year={2023}
}

@inproceedings{strubell2020energy,
  title={Energy and policy considerations for modern deep learning research},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={09},
  pages={13693--13696},
  year={2020}
}

@inproceedings{bender2021dangers,
  title={On the dangers of stochastic parrots: Can language models be too big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={610--623},
  year={2021}
}

@article{alizadeh2024analyzing,
  title={Analyzing the Energy and Accuracy of LLMs in Software Development},
  author={Alizadeh, Negar and Belchev, Boris and Saurabh, Nishant and Kelbert, Patricia and Castor, Fernando},
  journal={arXiv e-prints},
  pages={arXiv--2412},
  year={2024}
}

@misc{luscombe2024three, 
  title={Three Mile Island nuclear reactor to restart to power Microsoft AI operations},
  author={Luscombe, Richard},
  journal={The Guardian},
  year={2024},
  month={9},
  day={20},
  url={https://www.theguardian.com/environment/2024/sep/20/three-mile-island-nuclear-plant-reopen-microsoft}
}​

@misc{bbc2025ai, 
  title = {AI's Energy Appetite Sparks Environmental Concerns},
  author = {da Silva, João},
  journal = {BBC News},
  year = {2024},
  month = {10},
  day = {15},
  url = {https://www.bbc.com/news/articles/c748gn94k95o}
}​

@misc{wnn2025facebook, 
  title = {Facebook owner Meta seeks up to 4 GW nuclear capacity},
  author = {world nuclear news},
  journal = {world nuclear news},
  year = {2024},
  month = {12},
  day = {4},
  url = {https://www.world-nuclear-news.org/articles/facebook-owner-meta-seeks-up-to-4gw-nuclear-capacity}
}​

@misc{epoch2025energy,
	title = {How much energy does ChatGPT use?},
	author = {You, Josh},
  journal = {Epoch AI},
	year = {2025},
  month= {2},
  day = {7},
  url = {https://epoch.ai/gradient-updates/how-much-energy-does-chatgpt-use},
}

@article{alcott2005jevons,
  title={Jevons' paradox},
  author={Alcott, Blake},
  journal={Ecological economics},
  volume={54},
  number={1},
  pages={9--21},
  year={2005},
  publisher={Elsevier}
}

@misc{k2view2025GenAI,
	title = {GenAI adoption 2024: Challenges with enterprise data},
	author = {K2view},
  journal = {K2view},
  url = {https://www.k2view.com/genai-adoption-survey/},
  urldate = {2025-04-30}, 
}

@misc{menlov2025state,
	title = {2024: The State of Generative AI in the Enterprise},
	author = {Menlo Ventures},
  journal = {Menlo Ventures},
  url = {https://menlovc.com/2024-the-state-of-generative-ai-in-the-enterprise},
  urldate = {2025-04-30}, 
}

@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@article{li2024unseen,
  title={The unseen AI disruptions for power grids: LLM-induced transients},
  author={Li, Yuzhuo and Mughees, Mariam and Chen, Yize and Li, Yunwei Ryan},
  journal={arXiv preprint arXiv:2409.11416},
  year={2024}
}

@misc{ayers2025measuring,
	title = {Measuring the Emissions and Energy Footprint of the ICT Sector : Implications for Climate Action ( (English)},
	author = {Ayers, Seth and Ballan, Sara and Gray, Vanessa and McDonald Rosie},
  journal = {World Bank Group},
  url = {https://documents.worldbank.org/en/publication/documents-reports/documentdetail/099121223165540890/p17859702a98880540a4b70d57876048abb},
	year = {2024},
  month= {3},
  day = {21},
}

@misc{iea2025ai,
	title = {AI is set to drive surging electricity demand from data centres while offering the potential to transform how the energy sector works},
	author = {IEA},
  journal = {IEA},
  url = {https://www.iea.org/news/ai-is-set-to-drive-surging-electricity-demand-from-data-centres-while-offering-the-potential-to-transform-how-the-energy-sector-works},
	year = {2025},
  month= {4},
  day = {10},
}

@misc{espin2025rise,
	title = {The Rise of Local LLMs: Why Everyone Is Building Their Own AI Models},
	author = {E-SPIN},
  journal = {E-SPIN},
  url = {https://www.e-spincorp.com/rise-of-local-llms-ai-models},
	year = {2025},
  month= {2},
  day = {26},
}

@misc{pieces2025local,
	title = {Local large language models (LLMs) and their growing traction},
	author = {Pieces},
  journal = {Pieces},
  url = {https://pieces.app/blog/local-large-language-models-lllms-and-copilot-integrations},
	year = {2025},
  month= {3},
  day = {27},
}

@misc{envisioning2025gpu,
	title = {GPU-Poor},
	author = {Envisioning},
  journal = {Envisioning},
  url = {https://www.envisioning.io/vocab/gpu-poor},
	year = {2022},
}

@article{svirschevski2024specexec,
  title={Specexec: Massively parallel speculative decoding for interactive llm inference on consumer devices},
  author={Svirschevski, Ruslan and May, Avner and Chen, Zhuoming and Chen, Beidi and Jia, Zhihao and Ryabinin, Max},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={16342--16368},
  year={2024}
}

@misc{bamoria2025solving,
	title = {Solving Latency Challenges in LLM Deployment for Faster, Smarter Responses},
	author = {Bamoria, Himanshu},
  journal = {Medium},
  url = {https://medium.com/athina-ai/solving-latency-challenges-in-llm-deployment-for-faster-smarter-responses-64ff301e40d3},
	year = {2024},
  month= {9},
  day = {25},
}

@misc{chudleigh2025complete,
	title = {Complete Guide to LLM Agents (2025)},
	author = {Chudleigh, Sarah},
  journal = {Botpress},
  url = {https://botpress.com/blog/llm-agents},
	year = {2024},
  month= {11},
  day = {6},
}

@misc{suard2025quick,
	title = {A Quick Review of The Most Popular AI Agent Frameworks (June 2024)},
	author = {Suard, Tyler},
  journal = {Medium},
  url = {https://medium.com/@ceo_44783/a-quick-review-of-the-most-popular-ai-agent-frameworks-june-2024-ce53c0ef809a},
	year = {2024},
  month= {6},
  day = {20},
}

@misc{hassabis2025year,
	title = {2024: A year of extraordinary progress and advancement in AI},
	author = {Hassabis, Demis and Manyika, James and Dean, Jeff},
  journal = {Google},
  url = {https://blog.google/technology/ai/2024-ai-extraordinary-progress-advancement/},
	year = {2025},
  month= {1},
  day = {23},
}

@article{fernandez2025energy,
  title={Energy Considerations of Large Language Model Inference and Efficiency Optimizations},
  author={Fernandez, Jared and Na, Clara and Tiwari, Vashisth and Bisk, Yonatan and Luccioni, Sasha and Strubell, Emma},
  journal={arXiv preprint arXiv:2504.17674},
  year={2025}
}

@article{chen2024practical,
  title={Practical offloading for fine-tuning LLM on commodity GPU via learned subspace projectors},
  author={Chen, Siyuan and Guan, Zelong and Liu, Yudong and Gibbons, Phillip B},
  journal={arXiv preprint arXiv:2406.10181},
  year={2024}
}

@misc{ugent2025verantwoord,
	title = {VERANTWOORD GEBRUIK GENERATIEVE AI IN DE MASTERPROEF FACULTEIT INGENIEURSWETENSCHAPPEN EN ARCHITECTUUR},
	author = {UGent},
  journal = {UGent},
  url = {https://www.ugent.be/ea/nl/faculteit/studentenadministratie/masterproef/ai_2425.pdf},
	urldate = {2025-05-03}, 
}

@article{poddar2025towards,
  title={Towards Sustainable NLP: Insights from Benchmarking Inference Energy in Large Language Models},
  author={Poddar, Soham and Koley, Paramita and Misra, Janardan and Podder, Sanjay and Ganguly, Niloy and Ghosh, Saptarshi},
  journal={arXiv preprint arXiv:2502.05610},
  year={2025}
}

@misc{langchain2025pypdfdirectoryloader,
	title = {PyPDFDirectoryLoader},
	author = {langchain},
  journal = {langchain},
  url = {https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFDirectoryLoader.html},
}

@misc{langchain2025recursivecharactertextsplitter,
	title = {RecursiveCharacterTextSplitter},
	author = {langchain},
  journal = {langchain},
  url = {https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html},
}

@article{robertson2009probabilistic,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={4},
  pages={333--389},
  year={2009},
  publisher={Now Publishers, Inc.}
}

@article{muennighoff2022mteb,
  title={MTEB: Massive text embedding benchmark},
  author={Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"\i}c and Reimers, Nils},
  journal={arXiv preprint arXiv:2210.07316},
  year={2022}
}

@article{kim2023speculative,
  title={Speculative decoding with big little decoder},
  author={Kim, Sehoon and Mangalam, Karttikeya and Moon, Suhong and Malik, Jitendra and Mahoney, Michael W and Gholami, Amir and Keutzer, Kurt},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={39236--39256},
  year={2023}
}

@article{zhou2023distillspec,
  title={Distillspec: Improving speculative decoding via knowledge distillation},
  author={Zhou, Yongchao and Lyu, Kaifeng and Rawat, Ankit Singh and Menon, Aditya Krishna and Rostamizadeh, Afshin and Kumar, Sanjiv and Kagy, Jean-Fran{\c{c}}ois and Agarwal, Rishabh},
  journal={arXiv preprint arXiv:2310.08461},
  year={2023}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@inproceedings{devlin2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages={4171--4186},
  year={2019}
}

@misc{nginx,
  author       = {{NGINX, Inc.}},
  title        = {NGINX},
  year         = 2024,
  url          = {https://nginx.org},
  note         = {Accessed: 2025-05-18}
}

@misc{weaviate,
  author       = {{Weaviate}},
  title        = {Weaviate},
  year         = 2024,
  url          = {https://weaviate.io},
  note         = {Accessed: 2025-05-18}
}

@misc{postgresql,
  author       = {{PostgreSQL Global Development Group}},
  title        = {PostgreSQL},
  year         = 2024,
  url          = {https://www.postgresql.org},
  note         = {Accessed: 2025-05-18}
}

@misc{docker,
  author       = {{Docker, Inc.}},
  title        = {Docker},
  year         = 2024,
  url          = {https://www.docker.com},
  note         = {Accessed: 2025-05-18}
}

@article{grattafiori2024llama,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{frantar2022gptq,
  title={Gptq: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@inproceedings{xiao2023smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={International Conference on Machine Learning},
  pages={38087--38099},
  year={2023},
  organization={PMLR}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{aggarwal2024automix,
  title={Automix: Automatically mixing language models},
  author={Aggarwal, Pranjal and Madaan, Aman and Anand, Ankit and Potharaju, Srividya Pranavi and Mishra, Swaroop and Zhou, Pei and Gupta, Aditya and Rajagopal, Dheeraj and Kappaganthu, Karthik and Yang, Yiming and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={131000--131034},
  year={2024}
}
