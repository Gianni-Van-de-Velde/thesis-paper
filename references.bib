@article{EAGLE-2,
  author={Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang},
  year={2024},
  title={Eagle-2: Faster inference of language models with dynamic draft trees},
  journal={arXiv preprint arXiv:2406.16858},
}

@article{li2024eagle,
  title={Eagle-2: Faster inference of language models with dynamic draft trees},
  author={Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang},
  journal={arXiv preprint arXiv:2406.16858},
  year={2024}
}

@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{shareGPT,
  title={ShareGPT Dataset},
  author={Ryoko AI},
  url={https://huggingface.co/datasets/RyokoAI/ShareGPT52K},
}

@article{li2024eagle1,
  title={Eagle: Speculative sampling requires rethinking feature uncertainty},
  author={Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang},
  journal={arXiv preprint arXiv:2401.15077},
  year={2024}
}

@article{cai2024medusa,
  title={Medusa: Simple llm inference acceleration framework with multiple decoding heads},
  author={Cai, Tianle and Li, Yuhong and Geng, Zhengyang and Peng, Hongwu and Lee, Jason D and Chen, Deming and Dao, Tri},
  journal={arXiv preprint arXiv:2401.10774},
  year={2024}
}

@inproceedings{leviathan2023fast,
  title={Fast inference from transformers via speculative decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  booktitle={International Conference on Machine Learning},
  pages={19274--19286},
  year={2023},
  organization={PMLR}
}

@article{he2023rest,
  title={Rest: Retrieval-based speculative decoding},
  author={He, Zhenyu and Zhong, Zexuan and Cai, Tianle and Lee, Jason D and He, Di},
  journal={arXiv preprint arXiv:2311.08252},
  year={2023}
}

@article{qin2024optimized,
  title={Optimized Multi-Token Joint Decoding with Auxiliary Model for LLM Inference},
  author={Qin, Zongyue and Hu, Ziniu and He, Zifan and Prakriya, Neha and Cong, Jason and Sun, Yizhou},
  journal={arXiv preprint arXiv:2407.09722},
  year={2024}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

@article{khattab2022demonstrate,
  title={Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp},
  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2212.14024},
  year={2022}
}

@article{trivedi2022interleaving,
  title={Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions},
  author={Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2212.10509},
  year={2022}
}

@inproceedings{kim2023tree,
  title={Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models},
  author={Kim, Gangwoo and Kim, Sungdong and Jeon, Byeongguk and Park, Joonsuk and Kang, Jaewoo},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={996--1009},
  year={2023}
}

@article{rackauckas2024ragfusion,
  title={Rag-fusion: a new take on retrieval-augmented generation},
  author={Rackauckas, Zackary},
  journal={arXiv preprint arXiv:2402.03367},
  year={2024}
}

@article{zheng2023takeastepback,
  title={Take a step back: Evoking reasoning via abstraction in large language models},
  author={Zheng, Huaixiu Steven and Mishra, Swaroop and Chen, Xinyun and Cheng, Heng-Tze and Chi, Ed H and Le, Quoc V and Zhou, Denny},
  journal={arXiv preprint arXiv:2310.06117},
  year={2023}
}

@inproceedings{gao2023precisehyde,
  title={Precise zero-shot dense retrieval without relevance labels},
  author={Gao, Luyu and Ma, Xueguang and Lin, Jimmy and Callan, Jamie},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1762--1777},
  year={2023}
}

@inproceedings{formal2021splade,
  title={SPLADE: Sparse lexical and expansion model for first stage ranking},
  author={Formal, Thibault and Piwowarski, Benjamin and Clinchant, St{\'e}phane},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2288--2292},
  year={2021}
}

@article{formal2021spladev2,
  title={SPLADE v2: Sparse lexical and expansion model for information retrieval},
  author={Formal, Thibault and Lassance, Carlos and Piwowarski, Benjamin and Clinchant, St{\'e}phane},
  journal={arXiv preprint arXiv:2109.10086},
  year={2021}
}

@article{lassance2024spladev3,
  title={SPLADE-v3: New baselines for SPLADE},
  author={Lassance, Carlos and D{\'e}jean, Herv{\'e} and Formal, Thibault and Clinchant, St{\'e}phane},
  journal={arXiv preprint arXiv:2403.06789},
  year={2024}
}

@article{izacard2021unsupervisedcontriever,
  title={Unsupervised dense information retrieval with contrastive learning},
  author={Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
  journal={arXiv preprint arXiv:2112.09118},
  year={2021}
}

@article{ni2021largegtr,
  title={Large dual encoders are generalizable retrievers},
  author={Ni, Jianmo and Qu, Chen and Lu, Jing and Dai, Zhuyun and {\'A}brego, Gustavo Hern{\'a}ndez and Ma, Ji and Zhao, Vincent Y and Luan, Yi and Hall, Keith B and Chang, Ming-Wei and others},
  journal={arXiv preprint arXiv:2112.07899},
  year={2021}
}

@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@article{chen2024bge,
  title={Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation},
  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},
  journal={arXiv preprint arXiv:2402.03216},
  year={2024}
}

@article{zhuang2023qlm,
  title={Open-source large language models are strong zero-shot query likelihood models for document ranking},
  author={Zhuang, Shengyao and Liu, Bing and Koopman, Bevan and Zuccon, Guido},
  journal={arXiv preprint arXiv:2310.13243},
  year={2023}
}

@misc{reddit2025embedding,
  author = {{u/fictioninquire}},
  title = {Best open source embedding models for EU languages},
  year = {2024},
  url = {https://www.reddit.com/r/LocalLLaMA/comments/1chqkph/best_open_source_embedding_models_for_eu_languages/},
  note = {Reddit thread}
}

@article{remy2024biolord,
  title={BioLORD-2023: semantic textual representations fusing large language models and clinical knowledge graph insights},
  author={Remy, Fran{\c{c}}ois and Demuynck, Kris and Demeester, Thomas},
  journal={Journal of the American Medical Informatics Association},
  volume={31},
  number={9},
  pages={1844--1855},
  year={2024},
  publisher={Oxford University Press}
}

@article{levy2024sameinputlength,
  title={Same task, more tokens: the impact of input length on the reasoning performance of large language models},
  author={Levy, Mosh and Jacoby, Alon and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2402.14848},
  year={2024}
}

@article{kim2023arithmeticintensityllm,
  title={Full stack optimization of transformer inference: a survey},
  author={Kim, Sehoon and Hooper, Coleman and Wattanawong, Thanakul and Kang, Minwoo and Yan, Ruohan and Genc, Hasan and Dinh, Grace and Huang, Qijing and Keutzer, Kurt and Mahoney, Michael W and others},
  journal={arXiv preprint arXiv:2302.14017},
  year={2023}
}

@inproceedings{liskavets2025cpc,
  title={Prompt compression with context-aware sentence encoding for fast and improved llm inference},
  author={Liskavets, Barys and Ushakov, Maxim and Roy, Shuvendu and Klibanov, Mark and Etemad, Ali and Luke, Shane K},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={23},
  pages={24595--24604},
  year={2025}
}

@article{xu2023recomp,
  title={Recomp: Improving retrieval-augmented lms with compression and selective augmentation},
  author={Xu, Fangyuan and Shi, Weijia and Choi, Eunsol},
  journal={arXiv preprint arXiv:2310.04408},
  year={2023}
}

@article{li2023selectivecontext,
  title={Compressing context to enhance inference efficiency of large language models},
  author={Li, Yucheng and Dong, Bo and Lin, Chenghua and Guerin, Frank},
  journal={arXiv preprint arXiv:2310.06201},
  year={2023}
}

@article{jiang2023llmlingua,
  title={Llmlingua: Compressing prompts for accelerated inference of large language models},
  author={Jiang, Huiqiang and Wu, Qianhui and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
  journal={arXiv preprint arXiv:2310.05736},
  year={2023}
}

@article{pan2024llmlingua2,
  title={Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression},
  author={Pan, Zhuoshi and Wu, Qianhui and Jiang, Huiqiang and Xia, Menglin and Luo, Xufang and Zhang, Jue and Lin, Qingwei and R{\"u}hle, Victor and Yang, Yuqing and Lin, Chin-Yew and others},
  journal={arXiv preprint arXiv:2403.12968},
  year={2024}
}

@article{jiang2023longllmlingua,
  title={Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression},
  author={Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Li, Dongsheng and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
  journal={arXiv preprint arXiv:2310.06839},
  year={2023}
}